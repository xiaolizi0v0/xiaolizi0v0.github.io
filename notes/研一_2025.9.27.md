### Transformer剖析：通过核视角对Transformer注意力机制的统一理解: [[1908.11775\] Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel](https://arxiv.org/abs/1908.11775)

#### 注意力机制与核函数联系起来的灵感

这两种操作都会同时处理所有的输入并计算输入之间的相似度。

Transformer中将每一个序列元素定义为$x_i=(f_i,t_i)$，其中$f_i\in{F}$是时间$i$处的非时间特征，$t_i\in{T}$是时间特征。

#### 核特征空间

1.  Sequence Transformer：
   $$
   \mathcal{X}:=(\mathcal{F}\times\mathcal{T})
   $$
   其中$\mathcal{F}$是非位置特征空间，$\mathcal{T}$是序列中的位置的位置嵌入空间。

2.  Image Transformer ：
   $$
   \mathcal{X}:=(\mathcal{F}\times\mathcal{H}\times\mathcal{W})
   $$
   其中$\mathcal{F}$是非位置特征空间,$\mathcal{H}$是图像中高度的位置空间，,$\mathcal{W}$是图像中宽度的位置空间。

3.  Set Transformer：
   $$
   \mathcal{X}:=(\mathcal{F})
   $$

​	不存在任何位置信息

4.  Image Transformer ：

$$
\mathcal{X}:=(\mathcal{F}\times\mathcal{H}\times\mathcal{W}\times\mathcal{W})
$$

​	其中$\mathcal{F}$是非位置特征空间,$\mathcal{H}$是图像中高度的位置空间，,$\mathcal{W}$是图像中宽度的位置空间。