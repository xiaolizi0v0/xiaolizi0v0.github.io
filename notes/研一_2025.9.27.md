### Transformer剖析：通过核视角对Transformer注意力机制的统一理解: [[1908.11775\] Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel](https://arxiv.org/abs/1908.11775)

#### 注意力机制与核函数联系起来的灵感

这两种操作都会同时处理所有的输入并计算输入之间的相似度。

Transformer中将每一个序列元素定义为$x_i=(f_i,t_i)$，其中$f_i\in{F}$是时间$i$处的非时间特征，$t_i\in{T}$是时间特征。

#### 核特征空间

1.  Sequence Transformer：
   $$
   \mathcal{X}:=(\mathcal{F}\times\mathcal{T})
   $$
   其中$\mathcal{F}$是非位置特征空间，$\mathcal{T}$是序列中的位置的位置嵌入空间。

2.  Image Transformer ：
   $$
   \mathcal{X}:=(\mathcal{F}\times\mathcal{H}\times\mathcal{W})
   $$
   其中$\mathcal{F}$是非位置特征空间,$\mathcal{H}$是图像中高度的位置空间，,$\mathcal{W}$是图像中宽度的位置空间。

3.  Set Transformer：
   $$
   \mathcal{X}:=(\mathcal{F})
   $$

​	不存在任何位置信息

4.  Image Transformer ：

$$
\mathcal{X}:=(\mathcal{F}^\ell\times\mathcal{F}^v\times\mathcal{F}^a\times\mathcal{T})
$$

​	其中$\mathcal{F}^\ell$代表语言特征空间,$\mathcal{F}^v$代表视觉特征空间，$\mathcal{F}^a$代表音频特征空间，$T$代表时间指示符空间。

#### 核构造与位置嵌入

##### 在$X=(F\times{T})$上内内核构建。

- 绝对位置嵌入：每个$t_i$都由一个向量表示，其中 每个维度都是正弦或余弦函数。对于学习到的位置嵌入，每个$t_{i}$都是一个学习到的参数，并且在不同序列中对于相同位置是固定的。即$X=F\oplus{T}$。通过核的视角，核相似度定义为：
  $$
  k(x_{q},x_k):=k_{exp}(f_q+t_q,f_k+t_k)(1)
  $$

- TransformerXL中的相对位置嵌入：其核函数被选为混合正弦核余弦函数的非对称函数
  $$
  k(x_{q},x_k):=k_{exp}(f_q,f_k)\cdot{k_{f_q}(t_q,t_k)}(2)
  $$
  其中$kf_q(t_q,t_k)$是一个非对称核函数

- look-up table相对位置嵌入：
  $$
  k(x_{q},x_k):=L_{t_q-t_k,f_q}\cdot{k_{exp}(f_q,f_k)}(3)
  $$
  其中$L_{t_q-t_k,f_q}=exp(f_qW_{aq_t-t_k})$，a是一个可学习矩阵，其矩阵宽度为序列长度。且优化排名为：(2)>(3)>(1);

##### 价值函数

- 原始Transformer模型：
  $$
  v(x_k)=v((f_x,t_k)):=(f_k+t_k)W_{v}(1)
  $$
  
- Transformer-XL、Music Transformer、带相对嵌入的自注意力：
  $$
  v(x_k)=v((f_x,t_k)):=f_kW_{v}(2)
  $$
  其中(2)较(1)更优

##### 过滤函数

- 从核函数的角度去看**softmax函数可以作为一种概率函数**来实现，用于$x_q$观察$S_{x_k}$集合中的keys{$x_k$}。该概率由$x_q$与$x_k$之间的点积确定，附加映射$W_q/W_k$，并通过$d_k$进行缩放，且点积运算属于核函数的一种实例。而过滤函数$M(x_q,S_{x_k}):\mathcal{X\times{S}\rightarrow{S}}$，过滤函数返回一个集合，其元素与$x_q$进行运算（或connected/visble)。过滤函数M起到了**解码器自注意力中掩码**的作用。

​	从而注意力机制可重新定义为：
$$
Attention(x_q,M(x_q,S_{x_k}))=\sum_{x_k\in{M(x_q,S_{x_k})}}\frac{k(x_q,x_k)}{\sum_{x_{k^,}\in{M(x_q,S_{x_k})}}k(x_q,x_{k^,})}v(x_k)=\mathbb{E}_{p(x_k|x_q)}[v(x_k)]
$$
​	其中$v(x_k)$为价值函数，输出values，$p(x_k|x_q)=\frac{k(x_q,S_{x_k})}{\sum_{x_{k^,}\in{M(x_q,S_{x_k})}}k(x_q,x_{k^,})}$是一个概率，当核k始终为正时，函数取决于k和N。$k(x_q,x_k)=exp(<x_qW_q,x_kW_k>/\sqrt{d_k})$

过滤函数的变种：

1. 原始Transformer中编码器自注意力：对于编码序列中的每个查询$x_q$,$M(x_q,S_{x_k})=S_{x_k}$包含的key是编码序列中的所有标记。编码器自注意力考虑$x_q=x_k$,其中$x_q$是编码序列。
2. 原始Transformer中编码器-解码器注意力：过滤函数与1）中一样，不一样的是$x_q\neq{x_k}$，其中$x_q$是解码序列，$x_k$是编码序列。
3. 过滤函数$M(x_q,S_{x_k})$的返回值**为$S_{x_k}(M(x_q,S_{x_k})\subset{S_{x_k}})$的一个子集**。考虑了$x_q=x_k$
4. 过滤函数$M(x_q,S_{x_k})$的返回一个集合，**该集合包含$S_1$和额外的memories（$M(x_q,S_{x_k})=S_1+S_{mem},M(x_q,S_{x_k})\supset{S_1}$），其中$S_{mem}$指的是额外的记忆。**
5. 过滤函数$M(x_q,S_{x_k})$的**返回一个$S_1(M(x_q,S_{x_k})\subset{S_1})$的一个子集**。

##### 本文设计的核函数

具有以下特点：1）有效性（该核是对称的且半正定）2）在联合空间上构造核是精巧的（即$\mathcal{X=(F\times{T})}$）：
$$
k(x_q,x_k):=k_F(f_q,f_k)\cdot{k_T(t_q,t_k)}\ with\ k_F(f_q,f_k)=exp(\frac{<f_qW_F,f_kW_F>}{\sqrt{d_k}})\ and \ 
k_T(f_q,f_k)=exp(\frac{<f_qW_T,f_kW_T>}{\sqrt{d_k}})
$$
其中$W_F$和$W_T$是权重矩阵，第一个核用于度量非时间特征之间的相似性，第二个核用于度量时间特征之间的相似性。两个核都是对称指数核。

#### 实验一（PE对照，核类型对照）

<img src="..\image\image-20250928174043017.png" alt="image-20250928174043017" />

#### 实验二（注意力机制，价值函数嵌入）

<img src="..\image\image-20250928175136093.png" alt="image-20250928175136093" />

#### 结论

**乘积核（含对称与非对称）是最优的位置嵌入整合方式**，其中本文对称乘积核在 NMT 任务中表现最佳，在 SP 任务中与 Transformer-XL 持平。

**无限特征空间核（指数核、RBF 核）显著优于有限特征空间核（多项式核）**，线性核因可能产生负值违反核平滑器假设而不收敛；**对称核与非对称核性能差异极小**，但对称核可减少参数（如$W_q=W_k$），更具效率优势。

尽管解码器自注意力非顺序无关，但位置嵌入仍对整体性能至关重要，尤其在编码器和编码器 - 解码器注意力中。

**值函数中加入位置嵌入无性能增益，反而可能轻微下降**，因此建议值函数仅使用非位置特征（$v(f_k)=f_kW_v$）。

