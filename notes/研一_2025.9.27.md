### Transformer剖析：通过核视角对Transformer注意力机制的统一理解: [[1908.11775\] Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel](https://arxiv.org/abs/1908.11775)

#### 注意力机制与核函数联系起来的灵感

这两种操作都会同时处理所有的输入并计算输入之间的相似度。

Transformer中将每一个序列元素定义为$x_i=(f_i,t_i)$，其中$f_i\in{F}$是时间$i$处的非时间特征，$t_i\in{T}$是时间特征。

#### 核特征空间

1.  Sequence Transformer：
   $$
   \mathcal{X}:=(\mathcal{F}\times\mathcal{T})
   $$
   其中$\mathcal{F}$是非位置特征空间，$\mathcal{T}$是序列中的位置的位置嵌入空间。

2.  Image Transformer ：
   $$
   \mathcal{X}:=(\mathcal{F}\times\mathcal{H}\times\mathcal{W})
   $$
   其中$\mathcal{F}$是非位置特征空间,$\mathcal{H}$是图像中高度的位置空间，,$\mathcal{W}$是图像中宽度的位置空间。

3.  Set Transformer：
   $$
   \mathcal{X}:=(\mathcal{F})
   $$

​	不存在任何位置信息

4.  Image Transformer ：

$$
\mathcal{X}:=(\mathcal{F}^\ell\times\mathcal{F}^v\times\mathcal{F}^a\times\mathcal{T})
$$

​	其中$\mathcal{F}^\ell$代表语言特征空间,$\mathcal{F}^v$代表视觉特征空间，$\mathcal{F}^a$代表音频特征空间，$T$代表时间指示符空间。

#### 核构造与位置嵌入

##### 在$X=(F\times{T})$上内内核构建。

- 绝对位置嵌入：每个$t_i$都由一个向量表示，其中 每个维度都是正弦或余弦函数。对于学习到的位置嵌入，每个$t_{i}$都是一个学习到的参数，并且在不同序列中对于相同位置是固定的。即$X=F\oplus{T}$。通过核的视角，核相似度定义为：
  $$
  k(x_{q},x_k):=k_{exp}(f_q+t_q,f_k+t_k)(1)
  $$

- TransformerXL中的相对位置嵌入：其核函数被选为混合正弦核余弦函数的非对称函数
  $$
  k(x_{q},x_k):=k_{exp}(f_q,f_k)\cdot{kf_q(t_q,t_k)}(2)
  $$
  其中$kf_q(t_q,t_k)$是一个非对称核函数

- look-up table相对位置嵌入：
  $$
  k(x_{q},x_k):=L_{t_q-t_k,f_q}\cdot{k_{exp}(f_q,f_k)}(3)
  $$
  其中$L_{t_q-t_k,f_q}=exp(f_qW_{aq_t-t_k})$，a是一个可学习矩阵，其矩阵宽度为序列长度。且优化排名为：(2)>(3)>(1);

