### Informer---[[2012.07436v3\] Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436v3)

#### 摘要要点

Informer具有三个独特的特点：

1）提出了一种*ProbSparse*自注意力机制它在时间复杂度和内存使用方面实现了$O(L\space log⁡L)$，并且在序列依赖对齐方面具有可比的性能。

2）自注意力蒸馏通过将级联层输入减半来突出主要的注意力，并有效地处理极长的输入序列。

3）生成式解码器，虽然概念简单，但在一次前向操作中预测长序列时间序列，而不是一步一步地预测，这大大提高了长序列预测的推理速度。

#### 在Informer出现之前的Transformer优化模型

**Sparse Transformer、LogSparse Transformer、Longforme**r都使用启发式方法来解决第一个局限性，并将自注意力机制的复杂度降低到$O(L\space log⁡L)$

**Reformer**也使用局部敏感哈希自注意力机制实现了$O(L\ log⁡L)$，但它只适用于极长的序列。

**Linformer**声称具有线性复杂度$O(L)$，但其投影矩阵不能针对现实世界中的长序列输入固定，这可能存在退化为$O(L^2)$的风险。

**Transformer-XL**和**Compressive Transformer**使用辅助隐藏状态来捕获长程依赖关系，这可能会加剧**自注意力机制的二次计算复杂度**

#### 创新点

- 我们提出了Informer模型，成功地增强了LSTF问题中的预测能力，这验证了Transformer类模型在捕获长序列时间序列输出和输入之间个体长程依赖关系方面的潜在价值。
- 我们提出了*ProbSparse*自注意力机制来有效地替代传统的自注意力机制。 它在依赖关系对齐方面实现了$O(Llog⁡L)$的时间复杂度和$O(Llog⁡L)$的内存使用量。
- 我们提出了自注意力蒸馏操作，以优先考虑J堆叠层中的主要注意力分数，并将总空间复杂度大幅降低到$O((2−ϵ)Llog⁡L)$，这有助于接收长序列输入。 
- 我们提出了生成式解码器，只需一步前向传播即可获得长序列输出，同时避免了推理阶段的累积误差传播。

#### 高效的自注意力机制

自注意力概率的分布具有潜在的稀疏性，通过Kullback-Leibler 散度$KL(q||p)=ln\sum_{l=1}^{L_k}e^{q_ik_l^T/\sqrt{d}}-\frac{1}{L_K}\sum_{j=1}^{L_k}q_i k_j^T/\sqrt{d}=\ln{L_K}$来衡量"相似性"。去掉常数项，我们将第i个查询的稀疏性度量定义为
$$
M(q_i,K) = \ln{\sum_{j=1}^{L_K}e^{\frac{q_ik_j^T}{\sqrt{d}}}}-\frac{1}{L_K}\sum_{j=1}^{L_K}\frac{q_ik_j^T}{\sqrt{d}}
$$


<img src="..\image\image-20250927161954018.png" alt="image-20250927161954018" />

<img src="..\image\image-20250927163935646.png" alt="image-20250927163935646" />

![image-20250927180646482](..\image\image-20250927180646482.png)
