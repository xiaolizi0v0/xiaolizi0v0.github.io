### Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecastinghttps://arxiv.org/abs/2106.13008

#### æ‘˜è¦è¦ç‚¹

1ï¼‰ä¸ºäº†è§£å†³é•¿æœŸæœªæ¥å¤æ‚çš„æ—¶ç©ºæ¨¡å¼ï¼Œæˆ‘ä»¬æå‡ºäº†*Autoformer*ä½œä¸ºä¸€ç§åˆ†è§£æ¶æ„ï¼Œå¹¶è®¾è®¡äº†å†…éƒ¨åˆ†è§£å—ï¼Œä»¥èµ‹äºˆæ·±åº¦é¢„æµ‹æ¨¡å‹å†…åœ¨çš„æ¸è¿›å¼åˆ†è§£èƒ½åŠ›ã€‚

2ï¼‰æå‡ºäº†Auto-correlationæœºåˆ¶ï¼Œæ”¹æœºåˆ¶åœ¨åºåˆ—çº§åˆ«è¿›è¡Œä¾èµ–æ€§å‘ç°å’Œä¿¡æ¯èšåˆã€‚ æˆ‘ä»¬çš„æœºåˆ¶è¶…è¶Šäº†ä»¥å¾€çš„è‡ªæ³¨æ„åŠ›å®¶æ—ï¼Œå¯ä»¥åŒæ—¶æé«˜è®¡ç®—æ•ˆç‡å’Œä¿¡æ¯åˆ©ç”¨ç‡ã€‚

3ï¼‰Autoformeråœ¨é•¿æœŸè®¾ç½®ä¸‹å®ç°äº†38%çš„ç›¸å¯¹æ”¹è¿›

#### é‡è¦çš„æ–‡çŒ®

##### ç»å…¸æ—¶åºé¢„æµ‹

- Antti Sorjamaa, Jin Hao, Nima Reyhani, Yongnan Ji, and Amaury Lendasse. Methodology for long-term prediction of time series. Neurocomputing[Methodology for long-term prediction of time series - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0925231207001610)
- Renyi Chen and Molei Tao. Data-driven prediction of general hamiltonian dynamics via learning exactlysymplectic maps.[[2103.05632\] Data-driven Prediction of General Hamiltonian Dynamics via Learning Exactly-Symplectic Maps](https://arxiv.org/abs/2103.05632)

##### ARIMAï¼šé€šè¿‡å·®åˆ†å°†éå¹³ç¨³è¿‡ç¨‹è½¬æ¢ä¸ºå¹³ç¨³è¿‡ç¨‹æ¥è§£å†³é¢„æµ‹é—®é¢˜

- G. E. P. Box and Gwilym M. Jenkins. Time series analysis, forecasting and control[Time series analysis forecasting and control - Janacek - 2010 - Journal of Time Series Analysis - Wiley Online Library](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9892.2009.00643.x)
- George EP Box and Gwilym M Jenkins. Some recent advances in forecasting and control. J. R. Stat. Soc. (Series-C), 1968.[Some Recent Advances in Forecasting and Control | Journal of the Royal Statistical Society Series C: Applied Statistics | Oxford Academic](https://academic.oup.com/jrsssc/article/23/2/158/6953499?login=false)

##### æ»¤æ³¢æ–¹æ³•é¢„æµ‹

- Richard Kurle, Syama Sundar Rangapuram, Emmanuel de BÃ©zenac, Stephan GÃ¼nnemann, and Jan Gasthaus. Deep rao-blackwellised particle filters for time series forecasting.[Deep Rao-Blackwellised particle filters for time series forecasting | Proceedings of the 34th International Conference on Neural Information Processing Systems](https://dl.acm.org/doi/10.5555/3495724.3497013)
- Emmanuel de BÃ©zenac, Syama Sundar Rangapuram, Konstantinos Benidis, Michael Bohlke-Schneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson, Patrick Gallinari, and Tim Januschowski. Normalizing kalman filters for multivariate time series analysis[Normalizing kalman filters for multivariate time series analysis | Proceedings of the 34th International Conference on Neural Information Processing Systems](https://dl.acm.org/doi/10.5555/3495724.3495976)

##### å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)ï¼šæ¨¡æ‹Ÿæ—¶é—´åºåˆ—çš„æ—¶åºä¾èµ–æ€§

- Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka. A multi-horizon quantile recurrent forecasterhttps://arxiv.org/abs/1711.11053
- Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting.https://dl.acm.org/doi/10.5555/3327757.3327876
- Rose Yu, Stephan Zheng, Anima Anandkumar, and Yisong Yue. Long-term forecasting using tensor-train rnnshttps://arxiv.org/abs/1711.00073
- Danielle C Maddix, Yuyang Wang, and Alex Smola. Deep factors with gaussian processes for forecastinghttps://arxiv.org/abs/1812.00098

##### DeepARï¼šç»“åˆè‡ªå›å½’æ–¹æ³•å’Œ RNN æ¥æ¨¡æ‹Ÿæœªæ¥åºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒ

- David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. DeepAR: Probabilistic forecasting with autoregressive recurrent networks

##### åŸºäºæ³¨æ„åŠ›çš„ RNNï¼šå¼•å…¥æ—¶é—´æ³¨æ„åŠ›æ¥æ¢ç´¢ç”¨äºé¢„æµ‹çš„é•¿ç¨‹ä¾èµ–å…³ç³»

- Q. Yao, D. Song, H. Chen, C. Wei, and G. W. Cottrell. A dual-stage attention-based recurrent neural network for time series predictionhttps://arxiv.org/abs/1704.02971
- Shun-Yao Shih, Fan-Keng Sun, and Hung-yi Lee. Temporal pattern attention for multivariate time series forecastinghttps://arxiv.org/abs/1809.04206
- Huan Song, Deepta Rajan, Jayaraman Thiagarajan, and Andreas Spanias. Attend and diagnose: Clinical time series analysis using attention modelshttps://arxiv.org/abs/1711.03905

##### åŸºäºæ—¶é—´å·ç§¯ç½‘ç»œ (TCN) ï¼šè¯•å›¾ç”¨å› æœå·ç§¯æ¥æ¨¡æ‹Ÿæ—¶é—´å› æœå…³ç³»

- AÃ¤ron van den Oord, S. Dieleman, H. Zen, K. Simonyan, Oriol Vinyals, A. Graves, Nal Kalchbrenner, A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio.https://arxiv.org/abs/1609.03499
- Anastasia Borovykh, Sander Bohte, and Cornelis W Oosterlee. Conditional time series forecasting with convolutional neural networks.https://arxiv.org/abs/1703.04691
- Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.https://arxiv.org/abs/1803.01271
-  Rajat Sen, Hsiang-Fu Yu, and Inderjit S. Dhillon. Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting. https://dl.acm.org/doi/10.5555/3454287.3454722

##### Transformerå˜ç§ï¼šé€šè¿‡ä¼˜åŒ–è‡ªæ³¨æ„åŠ›æœºåˆ¶é™ä½å¤æ‚åº¦

**LogTranL**:å°†å±€éƒ¨å·ç§¯å¼•å…¥ Transformerï¼Œå¹¶æå‡º LogSparse æ³¨æ„åŠ›æ¥é€‰æ‹©éµå¾ªæŒ‡æ•°é€’å¢é—´éš”çš„æ—¶é—´æ­¥é•¿ï¼Œä»è€Œå°†å¤æ‚åº¦é™ä½åˆ° ğ’ª(L(logâ¡L)2)

**Reformer**:æå‡ºäº†å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ (LSH) æ³¨æ„åŠ›ï¼Œå¹¶å°†å¤æ‚åº¦é™ä½åˆ° ğ’ª(Llogâ¡L)

**Informer**:ä½¿ç”¨åŸºäº KL æ•£åº¦çš„ ProbSparse æ³¨æ„åŠ›æ‰©å±•äº† Transformerï¼Œä¹Ÿè¾¾åˆ°äº† ğ’ª(Llogâ¡L) çš„å¤æ‚åº¦

#### æ—¶é—´åºåˆ—åˆ†è§£

Autoformeråˆ©ç”¨åˆ†è§£ä½œä¸ºæ·±åº¦æ¨¡å‹çš„å†…éƒ¨æ¨¡å—ï¼Œå¯ä»¥åœ¨æ•´ä¸ªé¢„æµ‹è¿‡ç¨‹ä¸­é€æ­¥åˆ†è§£éšè—åºåˆ—ï¼ŒåŒ…æ‹¬è¿‡å»çš„åºåˆ—å’Œé¢„æµ‹çš„ä¸­é—´ç»“æœã€‚

æ–‡ä¸­è¿˜æåˆ°äº†ï¼Œåœ¨è¿‡å»çš„é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œåˆ†è§£æ€»æ˜¯ç”¨ä½œé¢„æµ‹æœªæ¥åºåˆ—ä¹‹å‰çš„å†å²åºåˆ—çš„*é¢„å¤„ç†*ï¼Œä¾‹å¦‚å…·æœ‰è¶‹åŠ¿å­£èŠ‚æ€§åˆ†è§£çš„**Prophet**ã€å…·æœ‰åŸºå‡½æ•°å±•å¼€çš„**NBEATS**å’Œå…·æœ‰çŸ©é˜µåˆ†è§£çš„**DeepGLO**ï¼Œç„¶è€Œï¼Œè¿™ç§é¢„å¤„ç†å—åˆ°å†å²åºåˆ—çš„ç®€å•åˆ†è§£æ•ˆæœçš„é™åˆ¶ï¼Œå¹¶ä¸”å¿½ç•¥äº†é•¿æœŸæœªæ¥åºåˆ—æ½œåœ¨æ¨¡å¼ä¹‹é—´çš„å±‚æ¬¡äº¤äº’ã€‚

#### åºåˆ—åˆ†è§£å—

æ–‡ä¸­é‡‡ç”¨ç§»åŠ¨å¹³å‡æ³•æ¥å¹³æ»‘å‘¨æœŸæ€§æ³¢åŠ¨å¹¶çªå‡ºé•¿æœŸè¶‹åŠ¿ã€‚å¯¹äºé•¿åº¦ä¸º$L$çš„è¾“å…¥åºåˆ—$\mathcal{X}\in\mathbb{R}^{L\times{d}}$ï¼Œè¿‡ç¨‹å¦‚ä¸‹ï¼š
$$
\begin{aligned}
\mathcal{X}_t &= \text{AvgPool}(\text{Padding}(\mathcal{X})) \\
\mathcal{X}_s &= \mathcal{X} - \mathcal{X}_t
\end{aligned}
$$
å…¶ä¸­$\mathcal{X}_s,\mathcal{X}_t\in\mathbb{R}^{L\times{d}}$åˆ†åˆ«è¡¨ç¤ºå­£èŠ‚æ€§éƒ¨åˆ†å’Œæå–çš„è¶‹åŠ¿å¾ªç¯éƒ¨åˆ†ã€‚ æˆ‘ä»¬é‡‡ç”¨**AvgPool**(â‹…)è¿›è¡Œç§»åŠ¨å¹³å‡ï¼Œå¹¶ä½¿ç”¨å¡«å……æ“ä½œä¿æŒåºåˆ—é•¿åº¦ä¸å˜ã€‚ æˆ‘ä»¬ä½¿ç”¨$\mathcal{X}_s,\mathcal{X}_t=\text{SeriesDecomp}(\mathcal{X})$æ¥æ€»ç»“ä¸Šè¿°ç­‰å¼ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å‹å†…éƒ¨å—ã€‚

<img src="..\image\image-20251019133440153.png" alt="image-20251019133440153" />

#### æ¶æ„å›¾çš„æœ€å·¦ä¾§çš„æ•°æ®è¾“å…¥

<img src="..\image\image-20251019133644526.png" alt="image-20251019133644526"  />

è®¾å®šç¼–ç å™¨çš„è¾“å…¥æ˜¯è¿‡å»çš„$I$ä¸ªæ—¶é—´æ­¥$\mathcal{X}_{en}\in\mathbb{R}^{I\times{d}}$,è§£ç å™¨çš„è¾“å…¥åŒ…å«å¾…å¾…ç»†åŒ–çš„å­£èŠ‚æ€§éƒ¨åˆ†$\mathcal{X}_{des}\in\mathbb{R}^{(\frac{I}{2}+O)\times{d}}$

