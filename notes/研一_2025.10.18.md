### Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecastinghttps://arxiv.org/abs/2106.13008

#### 摘要要点

1）为了解决长期未来复杂的时空模式，我们提出了*Autoformer*作为一种分解架构，并设计了内部分解块，以赋予深度预测模型内在的渐进式分解能力。

2）提出了Auto-correlation机制，改机制在序列级别进行依赖性发现和信息聚合。 我们的机制超越了以往的自注意力家族，可以同时提高计算效率和信息利用率。

3）Autoformer在长期设置下实现了38%的相对改进

#### 重要的文献

##### 经典时序预测

- Antti Sorjamaa, Jin Hao, Nima Reyhani, Yongnan Ji, and Amaury Lendasse. Methodology for long-term prediction of time series. Neurocomputing[Methodology for long-term prediction of time series - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0925231207001610)
- Renyi Chen and Molei Tao. Data-driven prediction of general hamiltonian dynamics via learning exactlysymplectic maps.[[2103.05632\] Data-driven Prediction of General Hamiltonian Dynamics via Learning Exactly-Symplectic Maps](https://arxiv.org/abs/2103.05632)

- G. E. P. Box and Gwilym M. Jenkins. Time series analysis, forecasting and control[Time series analysis forecasting and control - Janacek - 2010 - Journal of Time Series Analysis - Wiley Online Library](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9892.2009.00643.x)
- George EP Box and Gwilym M Jenkins. Some recent advances in forecasting and control. J. R. Stat. Soc. (Series-C), 1968.[Some Recent Advances in Forecasting and Control | Journal of the Royal Statistical Society Series C: Applied Statistics | Oxford Academic](https://academic.oup.com/jrsssc/article/23/2/158/6953499?login=false)

##### 滤波方法预测

- Richard Kurle, Syama Sundar Rangapuram, Emmanuel de Bézenac, Stephan Günnemann, and Jan Gasthaus. Deep rao-blackwellised particle filters for time series forecasting.[Deep Rao-Blackwellised particle filters for time series forecasting | Proceedings of the 34th International Conference on Neural Information Processing Systems](https://dl.acm.org/doi/10.5555/3495724.3497013)
- Emmanuel de Bézenac, Syama Sundar Rangapuram, Konstantinos Benidis, Michael Bohlke-Schneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson, Patrick Gallinari, and Tim Januschowski. Normalizing kalman filters for multivariate time series analysis[Normalizing kalman filters for multivariate time series analysis | Proceedings of the 34th International Conference on Neural Information Processing Systems](https://dl.acm.org/doi/10.5555/3495724.3495976)

##### 循环神经网络 (RNN)：模拟时间序列的时序依赖性

- Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka. A multi-horizon quantile recurrent forecasterhttps://arxiv.org/abs/1711.11053
- Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting.https://dl.acm.org/doi/10.5555/3327757.3327876
- Rose Yu, Stephan Zheng, Anima Anandkumar, and Yisong Yue. Long-term forecasting using tensor-train rnnshttps://arxiv.org/abs/1711.00073
- Danielle C Maddix, Yuyang Wang, and Alex Smola. Deep factors with gaussian processes for forecastinghttps://arxiv.org/abs/1812.00098

##### DeepAR：结合自回归方法和 RNN 来模拟未来序列的概率分布

- David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. DeepAR: Probabilistic forecasting with autoregressive recurrent networks

##### 基于注意力的 RNN：引入时间注意力来探索用于预测的长程依赖关系

- Q. Yao, D. Song, H. Chen, C. Wei, and G. W. Cottrell. A dual-stage attention-based recurrent neural network for time series predictionhttps://arxiv.org/abs/1704.02971
- Shun-Yao Shih, Fan-Keng Sun, and Hung-yi Lee. Temporal pattern attention for multivariate time series forecastinghttps://arxiv.org/abs/1809.04206
- Huan Song, Deepta Rajan, Jayaraman Thiagarajan, and Andreas Spanias. Attend and diagnose: Clinical time series analysis using attention modelshttps://arxiv.org/abs/1711.03905

##### 基于时间卷积网络 (TCN) ：试图用因果卷积来模拟时间因果关系

- Aäron van den Oord, S. Dieleman, H. Zen, K. Simonyan, Oriol Vinyals, A. Graves, Nal Kalchbrenner, A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio.https://arxiv.org/abs/1609.03499
- Anastasia Borovykh, Sander Bohte, and Cornelis W Oosterlee. Conditional time series forecasting with convolutional neural networks.https://arxiv.org/abs/1703.04691
- Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.https://arxiv.org/abs/1803.01271
-  Rajat Sen, Hsiang-Fu Yu, and Inderjit S. Dhillon. Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting. https://dl.acm.org/doi/10.5555/3454287.3454722

##### Transformer变种：通过优化自注意力机制降低复杂度

**LogTranL**:将局部卷积引入 Transformer，并提出 LogSparse 注意力来选择遵循指数递增间隔的时间步长，从而将复杂度降低到 𝒪(L(log⁡L)2)

**Reformer**:提出了局部敏感哈希 (LSH) 注意力，并将复杂度降低到 𝒪(Llog⁡L)

**Informer**:使用基于 KL 散度的 ProbSparse 注意力扩展了 Transformer，也达到了 𝒪(Llog⁡L) 的复杂度

#### 时间序列分解

Autoformer利用分解作为深度模型的内部模块，可以在整个预测过程中逐步分解隐藏序列，包括过去的序列和预测的中间结果。

文中还提到了，在过去的预测任务中，分解总是用作预测未来序列之前的历史序列的*预处理*，例如具有趋势季节性分解的**Prophet**、具有基函数展开的**NBEATS**和具有矩阵分解的**DeepGLO**，然而，这种预处理受到历史序列的简单分解效果的限制，并且忽略了长期未来序列潜在模式之间的层次交互。

#### 序列分解块

文中采用移动平均法来平滑周期性波动并突出长期趋势。对于长度为$L$的输入序列$\mathcal{X}\in\mathbb{R}^{L\times{d}}$，过程如下：
$$
\begin{aligned}
\mathcal{X}_t &= \text{AvgPool}(\text{Padding}(\mathcal{X})) \\\\
\mathcal{X}_s &= \mathcal{X} - \mathcal{X}_t
\end{aligned}
$$
其中$\mathcal{X}_s,\mathcal{X}_t\in\mathbb{R}^{L\times{d}}$分别表示季节性部分和提取的趋势循环部分。 我们采用**AvgPool**(⋅)进行移动平均，并使用填充操作保持序列长度不变。 我们使用$\mathcal{X}_s,\mathcal{X}_t=\text{SeriesDecomp}(\mathcal{X})$来总结上述等式，这是一个模型内部块。

<img src="..\image\image-20251019133440153.png" alt="image-20251019133440153" />

#### 架构图的最左侧的数据输入

<img src="..\image\image-20251019133644526.png" alt="image-20251019133644526"  />

设定编码器的输入是过去的$I$个时间步$\mathcal{X}_{en}\in{\mathbb{R}^{I\times{d}}}$，解码器的输入包含待待细化的季节性部分$\mathcal{X}_{des}\in{\mathbb{R}^{(\frac{I}{2}+O)\times{d}}}$和趋势循环部分$\mathcal{X}_{det}\in{\mathbb{R}^{(\frac{I}{2}+O)\times{d}}}$。公式如下：
$$
\begin{aligned}
\mathcal{X}_{ens},\mathcal{X}_{ent} &= \text{SeriesDecomp}(\mathcal{X}_{en\frac{I}{2}:I})\\\\
\mathcal{X}_{des}&= \text{Concat}(\mathcal{X}_{ens},\mathcal{X}_{0})\\\\
\mathcal{X}_{det}&= \text{Concat}(\mathcal{X}_{ent},\mathcal{X}_{\text{Mean}})
\end{aligned}
$$
其中$\mathcal{X}_{0},\mathcal{X}_{\text{Mean}}\in{\mathbb{R}^{O\times{d}}}$分别表示用零和$\mathcal{X}_{en}$的均值填充的占位符

#### 架构图的上方的编码器

<img src="..\image\image-20251019152642427.png" alt="image-20251019152642427" />

定义第$l$个编码层的整体方程为$\mathcal{X}_{en}^l=\text{Encoder}(\mathcal{X}_{en}^{l-1})$。详情如下：
$$
\begin{align*}
\mathcal{S}_{en}^{l,1},\_ &= \text{SeriesDecomp}(\text{Auto-Correlation}(\mathcal{X}_{en}^{l-1})+\mathcal{X}_{en}^{l-1})\\\\
\mathcal{S}_{en}^{l,2},\_ &= \text{SeriesDecomp}(\text{FeedForward}(\mathcal{S}_{en}^{l,1})+\mathcal{S}_{en}^{l,1})
\end{align*}
$$
**SeriesDecomp**用来消除长期趋势循环部分，并专注于季节性模式建模。

其中\_是被消除的趋势部分。$\mathcal{X}_{en}^{l}=\mathcal{S}_{en}^{l,2}\in{1,\cdots,N}$表示第$l$个编码器层的输出，$\mathcal{X}_{en}^{0}$是嵌入的$\mathcal{X}_{en}$。￥$\mathcal{S}_{en}^{l,i},i\in{\{1,2\}}$分别表示第$l$层中第$i$个序列分解块后的季节性成分

#### 架构图的上方的解码器

<img src="..\image\image-20251019161633131.png" alt="image-20251019161633131" />

定义第$l$个解码器的方程总结为$\mathcal{X}_{de}^l=\text{Decoder}(\mathcal{X}_{de}^{l-1},\mathcal{X}_{en}^{N})$。解码器可以形式化为如下：
$$
\begin{align*}
\mathcal{S}_{de}^{l,1},\mathcal{F}_{de}^{l,1} &= \text{SeriesDecomp}(\text{Auto-Correlation}(\mathcal{X}_{de}^{l-1})+\mathcal{X}_{de}^{l-1})\\\\
\mathcal{S}_{de}^{l,2},\mathcal{F}_{de}^{l,2} &= \text{SeriesDecomp}(\text{Auto-Correlation}(\mathcal{S}_{de}^{l,1},\mathcal{X}_{en}^{N})+\mathcal{S}_{de}^{l,1})\\\\
\mathcal{S}_{de}^{l,3},\mathcal{F}_{de}^{l,3} &= \text{SeriesDecomp}(\text{
FeedForward}(\mathcal{S}_{de}^{l,2})+\mathcal{S}_{de}^{l,2})\\\\
\mathcal{F}_{de}^{l} &= \mathcal{F}_{de}^{l-1}+\mathcal{W}_{l,1}*\mathcal{F}_{de}^{l,1}+\mathcal{W}_{l,2}*\mathcal{F}_{de}^{l,2}+\mathcal{W}_{l,3}*\mathcal{F}_{de}^{l,3}
\end{align*}
$$
其中$\mathcal{X}_{de}^l=\mathcal{S}_{de}^{l,3},l∈\{1,⋯,M\}$表示l解码器层的输出。 $\mathcal{X}_{de}^0$从$\mathcal{X}_{des}$嵌入用于深度变换，而$\mathcal{F}_{de}^{0}=\mathcal{X}_{det}$用于累积。 $\mathcal{S}_{de}^{l,i},\mathcal{F}_{de}^{l,i},i∈\{1,2,3\}$分别表示第$l$层中第$i$个序列分解块后的季节性成分和趋势-周期性成分。 $\mathcal{W}_{l,i},i∈\{1,2,3\}$表示i的投影仪提取的趋势$\mathcal{F}_{de}^{l,i}$。

最终预测是两个细化分解成分的总和，如$\mathcal{W}_{\mathcal{S}}∗\mathcal{X}_{de}^M+\mathcal{F}_{de}^M$所示，其中$\mathcal{W}_S$将深度变换后的季节性成分$\mathcal{X}_{de}^M$投影到目标维度。

#### Auto-Correlation 自相关

<img src="..\image\image-20251019183022502.png" alt="image-20251019183022502" />

<img src="..\image\image-20251019185036054.png" alt="image-20251019185036054" />

<img src="..\image\image-20251019185115603.png" alt="image-20251019185115603" />

![img](https://github.com/thuml/Autoformer/raw/main/pic/results.png)
