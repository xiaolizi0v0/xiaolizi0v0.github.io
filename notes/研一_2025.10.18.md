### Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecastinghttps://arxiv.org/abs/2106.13008

#### 摘要要点

1）为了解决长期未来复杂的时空模式，我们提出了*Autoformer*作为一种分解架构，并设计了内部分解块，以赋予深度预测模型内在的渐进式分解能力。

2）提出了Auto-correlation机制，改机制在序列级别进行依赖性发现和信息聚合。 我们的机制超越了以往的自注意力家族，可以同时提高计算效率和信息利用率。

3）Autoformer在长期设置下实现了38%的相对改进

#### 重要的文献

##### 经典时序预测

- Antti Sorjamaa, Jin Hao, Nima Reyhani, Yongnan Ji, and Amaury Lendasse. Methodology for long-term prediction of time series. Neurocomputing[Methodology for long-term prediction of time series - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0925231207001610)
- Renyi Chen and Molei Tao. Data-driven prediction of general hamiltonian dynamics via learning exactlysymplectic maps.[[2103.05632\] Data-driven Prediction of General Hamiltonian Dynamics via Learning Exactly-Symplectic Maps](https://arxiv.org/abs/2103.05632)

##### ARIMA：通过差分将非平稳过程转换为平稳过程来解决预测问题

- G. E. P. Box and Gwilym M. Jenkins. Time series analysis, forecasting and control[Time series analysis forecasting and control - Janacek - 2010 - Journal of Time Series Analysis - Wiley Online Library](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9892.2009.00643.x)
- George EP Box and Gwilym M Jenkins. Some recent advances in forecasting and control. J. R. Stat. Soc. (Series-C), 1968.[Some Recent Advances in Forecasting and Control | Journal of the Royal Statistical Society Series C: Applied Statistics | Oxford Academic](https://academic.oup.com/jrsssc/article/23/2/158/6953499?login=false)

##### 滤波方法预测

- Richard Kurle, Syama Sundar Rangapuram, Emmanuel de Bézenac, Stephan Günnemann, and Jan Gasthaus. Deep rao-blackwellised particle filters for time series forecasting.[Deep Rao-Blackwellised particle filters for time series forecasting | Proceedings of the 34th International Conference on Neural Information Processing Systems](https://dl.acm.org/doi/10.5555/3495724.3497013)
- Emmanuel de Bézenac, Syama Sundar Rangapuram, Konstantinos Benidis, Michael Bohlke-Schneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson, Patrick Gallinari, and Tim Januschowski. Normalizing kalman filters for multivariate time series analysis[Normalizing kalman filters for multivariate time series analysis | Proceedings of the 34th International Conference on Neural Information Processing Systems](https://dl.acm.org/doi/10.5555/3495724.3495976)

##### 循环神经网络 (RNN)：模拟时间序列的时序依赖性

- Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka. A multi-horizon quantile recurrent forecasterhttps://arxiv.org/abs/1711.11053
- Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting.https://dl.acm.org/doi/10.5555/3327757.3327876
- Rose Yu, Stephan Zheng, Anima Anandkumar, and Yisong Yue. Long-term forecasting using tensor-train rnnshttps://arxiv.org/abs/1711.00073
- Danielle C Maddix, Yuyang Wang, and Alex Smola. Deep factors with gaussian processes for forecastinghttps://arxiv.org/abs/1812.00098

##### DeepAR：结合自回归方法和 RNN 来模拟未来序列的概率分布

- David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. DeepAR: Probabilistic forecasting with autoregressive recurrent networks

##### 基于注意力的 RNN：引入时间注意力来探索用于预测的长程依赖关系

- Q. Yao, D. Song, H. Chen, C. Wei, and G. W. Cottrell. A dual-stage attention-based recurrent neural network for time series predictionhttps://arxiv.org/abs/1704.02971
- Shun-Yao Shih, Fan-Keng Sun, and Hung-yi Lee. Temporal pattern attention for multivariate time series forecastinghttps://arxiv.org/abs/1809.04206
- Huan Song, Deepta Rajan, Jayaraman Thiagarajan, and Andreas Spanias. Attend and diagnose: Clinical time series analysis using attention modelshttps://arxiv.org/abs/1711.03905

##### 基于时间卷积网络 (TCN) ：试图用因果卷积来模拟时间因果关系

- Aäron van den Oord, S. Dieleman, H. Zen, K. Simonyan, Oriol Vinyals, A. Graves, Nal Kalchbrenner, A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio.https://arxiv.org/abs/1609.03499
- Anastasia Borovykh, Sander Bohte, and Cornelis W Oosterlee. Conditional time series forecasting with convolutional neural networks.https://arxiv.org/abs/1703.04691
- Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.https://arxiv.org/abs/1803.01271
-  Rajat Sen, Hsiang-Fu Yu, and Inderjit S. Dhillon. Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting. https://dl.acm.org/doi/10.5555/3454287.3454722

##### Transformer变种：通过优化自注意力机制降低复杂度

**LogTranL**:将局部卷积引入 Transformer，并提出 LogSparse 注意力来选择遵循指数递增间隔的时间步长，从而将复杂度降低到 𝒪(L(log⁡L)2)

**Reformer**:提出了局部敏感哈希 (LSH) 注意力，并将复杂度降低到 𝒪(Llog⁡L)

**Informer**:使用基于 KL 散度的 ProbSparse 注意力扩展了 Transformer，也达到了 𝒪(Llog⁡L) 的复杂度

#### 时间序列分解

Autoformer利用分解作为深度模型的内部模块，可以在整个预测过程中逐步分解隐藏序列，包括过去的序列和预测的中间结果。

文中还提到了，在过去的预测任务中，分解总是用作预测未来序列之前的历史序列的*预处理*，例如具有趋势季节性分解的**Prophet**、具有基函数展开的**NBEATS**和具有矩阵分解的**DeepGLO**，然而，这种预处理受到历史序列的简单分解效果的限制，并且忽略了长期未来序列潜在模式之间的层次交互。

#### 序列分解块

文中采用移动平均法来平滑周期性波动并突出长期趋势。对于长度为$L$的输入序列$\mathcal{X}\in\mathbb{R}^{L\times{d}}$，过程如下：
$$
\begin{aligned}
\mathcal{X}_t &= \text{AvgPool}(\text{Padding}(\mathcal{X})) \\
\mathcal{X}_s &= \mathcal{X} - \mathcal{X}_t
\end{aligned}
$$
其中$\mathcal{X}_s,\mathcal{X}_t\in\mathbb{R}^{L\times{d}}$分别表示季节性部分和提取的趋势循环部分。 我们采用**AvgPool**(⋅)进行移动平均，并使用填充操作保持序列长度不变。 我们使用$\mathcal{X}_s,\mathcal{X}_t=\text{SeriesDecomp}(\mathcal{X})$来总结上述等式，这是一个模型内部块。

<img src="..\image\image-20251019133440153.png" alt="image-20251019133440153" />

#### 架构图的最左侧的数据输入

<img src="..\image\image-20251019133644526.png" alt="image-20251019133644526"  />

设定编码器的输入是过去的$I$个时间步$\mathcal{X}_{en}\in\mathbb{R}^{I\times{d}}$,解码器的输入包含待待细化的季节性部分$\mathcal{X}_{des}\in\mathbb{R}^{(\frac{I}{2}+O)\times{d}}$

