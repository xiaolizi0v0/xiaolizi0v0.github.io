### DiM: Improving multivariate time series forecasting with DI embedding andmulti-head graph learning mechanism[DiM: Improving multivariate time series forecasting with DI embedding and multi-head graph learning mechanism - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S092523122502449X#preview-section-references)
#### 多元时间序列（MTS）

它由多个单变量时间序列组成，每个单变量时间序列代表一个具有独特时间动态的独立通道。此外，这些通过通过复杂的关系相互关联。如下图所示。

<img src="..\image\image-20251102204641810.png" alt="image-20251102204641810" />

#### 本文贡献

1.提出了一种DI时间嵌入策略，该策略将差异嵌入与现有的方向嵌入方法相结合，增强了模型捕捉时间动态的能力。

2.针对当前图学习的局限性，收多头注意力的启发，开发了一种多头图学习机制，能够更准确地识别通道间的关系，为传统的基于注意力的方法提供了一种可靠的替代方案

3.提出了DiM，是一种基于图神经网络（GNN）的多变量时间序列预测模型，它融合了DI嵌入和多头图学习机制。

#### 多变量时间序列预测嵌入方法

**Informer** , **Autoformer** , **Pyraformer** , and **FEDformer** 只是简单的在一个时间步的所有变量映射到高维空间中，只是简单的把变量做合并，可能会引入噪声并降低性能。为了解决这个问题，有人提出了**PatchTST**采用了一种补丁嵌入方法。而**iTransformer**提出了一种倒置嵌入方法。然而这些方法主要依赖线性映射，可能无法捕捉应时间影响下的动态通道关系。为了更能够获得通道之间的时间信息，后续讲究人员引入了注意力嵌入，如**sTransformer**通过时间卷积网络TCN替代线性映射，解决 了倒置嵌入方法的缺点。同时，**MTPNet**提出了一种维度不变的卷积神经网络CNN嵌入，通过卷积操作明确保留跨通道的时间动态和空间依赖性。此外，**PRfomer**利用金字塔循环卷积网络RNN嵌入来获取融合多尺度依赖信息的时间序列表示，增强了模型捕捉复杂时间动态的能力。但是本身的计算需求较大。

#### 多时间序列预测的通道间关系建模

解耦通道之间复杂的依赖关系对于多变量时间序列预测至关重要，**LSTNet**采用卷积神经网络捕捉通道之间关系，并利用循环神经解决时间依赖关系。**TSMixer**和**MTS-Mixers**则使用多层感知器同时解决两种依赖关系。基于Transformer的模型，如iTransformer,Crossforner和CSformer,借助注意力机制有效分析通道关系。本文引用了多头图学习机制来解决通道关系。

#### 模型架构

<img src="..\image\image-20251103172856899.png" alt="image-20251103172856899" />

Dim架构包括四个重要组件：实例归一化、DI嵌入、编码器和投影

##### DI Embedding

<img src="..\image\image-20251103192241743.png" alt="image-20251103192241743" />

差分嵌入可表示为：

<img src="..\image\image-20251103191140792.png" alt="image-20251103191140792" />



<img src="..\image\image-20251103191259316.png" alt="image-20251103191259316" />

DI嵌入，用一个超参数$p$调整差分嵌入与方向嵌入的捕捉即使变化与识别持久模式之间的平衡，可表示为：

<img src="..\image\image-20251103192059390.png" alt="image-20251103192059390" />

<img src="..\image\image-20251103192209422.png" alt="image-20251103192209422" />

##### Encode

<img src="..\image\image-20251103193348634.png" alt="image-20251103193348634" />

1.图学习，用图结构中的邻接矩阵来表述通道之间的依赖关系。在Dim中，邻接矩阵是在端到端范式中学习的。单向图结构的学习公式为：

<img src="..\image\image-20251103193706531.png" alt="image-20251103193706531" />

<img src="..\image\image-20251103191259316.png" alt="image-20251103191259316" />

<img src="..\image\image-20251103194646149.png" alt="image-20251103194646149" />

2多头图学习机制

多头学习机制的表示为：

<img src="..\image\image-20251103194851450.png" alt="image-20251103194851450" />

<img src="..\image\image-20251103194923928.png" alt="image-20251103194923928" />

<img src="..\image\image-20251103195017836.png" alt="image-20251103195017836" />